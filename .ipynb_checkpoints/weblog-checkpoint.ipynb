{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input log file into RDD object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2015-07-22T09:00:28.019143Z marketpalce-shop 123.242.248.130:54635 10.0.6.158:80 0.000022 0.026109 0.00002 200 200 0 699 \"GET https://paytm.com:443/shop/authresponse?code=f2405b05-e2ee-4b0d-8f6a-9fed0fcfe2e0&state=null HTTP/1.1\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36\" ECDHE-RSA-AES128-GCM-SHA256 TLSv1.2']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrame, Row, Column, Window\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "# jupyter notebook is configured with SparkContext instance\n",
    "# Cannot create another instance, hence commenting the following\n",
    "conf = SparkConf()\n",
    "#sc = SparkContext(conf=conf)\n",
    "\n",
    "# Create SparkSession (Entry point for pyspark Sql and DataFrame)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Weblog\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# Read input log file in spark context and create RDD object of input data\n",
    "log_data = sc.textFile(\"data/2015_07_22_mktplace_shop_web_log_sample.log.gz\")\n",
    "\n",
    "# Print test output of the log file\n",
    "str1 = log_data.take(1)\n",
    "print(str1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(client_ip=u'123.242.248.130', hour=9, minute=0, timestamp=1437555628, url=u'https://paytm.com:443/shop/authresponse?code=f2405b05-e2ee-4b0d-8f6a-9fed0fcfe2e0&state=null')\n"
     ]
    }
   ],
   "source": [
    "# Extracts meaningful attributes from a line of the log file \n",
    "# and adds them to a pyspark sql Row object\n",
    "\n",
    "def LineToRow(line):\n",
    "    words = line.split(\" \")\n",
    "\n",
    "    # Convert time to TimeStamp format\n",
    "    datetime_time = datetime.datetime.strptime(words[0], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    # Convert time to timestamp format\n",
    "    timestamp = calendar.timegm(datetime_time.utctimetuple())                 \n",
    "    \n",
    "    # Extract IP and port of client\n",
    "    client_ip, client_port = words[2].split(\":\")\n",
    "    \n",
    "    # Create Row object\n",
    "    row = Row(timestamp = timestamp,           # Timestamp\n",
    "              hour = datetime_time.hour,       # Hour from the timestamp\n",
    "              minute = datetime_time.minute,   # Minute from the timestamp\n",
    "              client_ip = client_ip, \n",
    "              url = words[12]) \n",
    "    \n",
    "    return row\n",
    "\n",
    "# Test the Row conversion method on first line of log file\n",
    "print(LineToRow(str1[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define schema for the log DataFrame\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"timestamp\", LongType(), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"minute\", IntegerType(), True),\n",
    "        StructField(\"client_ip\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map lines of log data to Row objects\n",
    "row_log_data = log_data.map(lambda row: LineToRow(row))\n",
    "\n",
    "# Create DataFrame from Row objects with the schema\n",
    "log_df = spark.createDataFrame(row_log_data, schema)\n",
    "\n",
    "# Create a Window Specification object for IP-wise operations on the data frame\n",
    "window_spec = Window.partitionBy(log_df.client_ip).orderBy(log_df.timestamp)\n",
    "\n",
    "# Add columns in the DataFrame for previous and next timestamp of a give IP\n",
    "# If the prev or next time is not present, i.e. the current timestamp is either\n",
    "# first or last respectively in the log file then the column value will be None\n",
    "# Here we apply 'lag' and 'lead' fuctions repectively on the Window partitioned\n",
    "# by IP address to get previous and next timestamps for the given IP\n",
    "# The resultant DataFrame is ordered by the timestamp at the end\n",
    "df_prev_next = log_df.withColumn('prev_timestamp', func.lag(log_df.timestamp, 1).over(window_spec))\\\n",
    ".withColumn('next_timestamp', func.lead(log_df.timestamp, 1).over(window_spec))\\\n",
    ".orderBy(log_df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add column for Time Since Prev Hit and Time Until Next Hit.\n",
    "# Time Since Prev Hit is difference between current timestamp \n",
    "# and previous timestamp.\n",
    "# Time Until Next Hit is difference between next timestamp and\n",
    "# current timestamp.\n",
    "\n",
    "df_prev_next = df_prev_next.withColumn('time_since_prev', df_prev_next.timestamp - df_prev_next.prev_timestamp)\\\n",
    ".withColumn('time_till_nxt', df_prev_next.next_timestamp - df_prev_next.timestamp)\\\n",
    ".orderBy(df_prev_next.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, select all the Rows which correspond to first or last\n",
    "# or both first and last appearance of an IP in a session\n",
    "\n",
    "# Assumption: If the user is inactive for more than 15 mins,\n",
    "# then assume the session length to be 15 mins\n",
    "MaxSessionTime = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A user appears for the first time in a session if\n",
    "##### 1. time_since_prev is None (In this case, this is  the first appearance of the user in the logs also), or\n",
    "##### 2. time_since_prev is more than MaxSessionTime\n",
    "\n",
    "### A user appears for the last time in a session if\n",
    "##### 1. time_till_nxt is None (In this case this is the last appearance of the user in the logs also), or\n",
    "##### 2. time_till_nxt is more than MaxSessionTime\n",
    "\n",
    "### A user appears for the first as well as last time\n",
    "##### in a session, i.e. the appearance is only appearance in that session if both of the following are True\n",
    "##### 1. time_since_prev is either None or more than MaxSessionTime\n",
    "##### 2. time_till_nxt is either None or more than MaxSessionTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting rows for which time_since_prev and time_till_nxt \n",
    "# are more than 900 or null. Since only these rows correspond to\n",
    "# first and last appearance of an IP in a session\n",
    "df_prev_next = df_prev_next.filter(func.isnull(df_prev_next.time_since_prev) | (df_prev_next.time_since_prev > MaxSessionTime) |\\\n",
    "                       (func.isnull(df_prev_next.time_till_nxt)) | (df_prev_next.time_till_nxt > MaxSessionTime))\\\n",
    "                        .orderBy(df_prev_next.timestamp)\n",
    "\n",
    "# Conditions for adding a column in the DataFrame, which indicates\n",
    "# if a session request is first or last or only appearance in the DataFrama\n",
    "\n",
    "col_filter = func\\\n",
    "    .when((func.isnull('time_since_prev') & func.isnull('time_till_nxt')), 'FirstAndLast')\\\n",
    "    .when((func.isnull('time_since_prev') & (df_prev_next.time_till_nxt > MaxSessionTime)), 'FirstAndLast')\\\n",
    "    .when((func.isnull('time_since_prev') & (df_prev_next.time_till_nxt < MaxSessionTime)), 'First')\\\n",
    "    .when(((df_prev_next.time_since_prev > MaxSessionTime) & func.isnull('time_till_nxt')), 'FirstAndLast')\\\n",
    "    .when(((df_prev_next.time_since_prev > MaxSessionTime) & (df_prev_next.time_till_nxt < MaxSessionTime)), 'First')\\\n",
    "    .when(((df_prev_next.time_since_prev > MaxSessionTime) & (df_prev_next.time_till_nxt > MaxSessionTime)), 'FirstAndLast')\\\n",
    "    .when(((df_prev_next.time_since_prev < MaxSessionTime) & func.isnull('time_till_nxt')), 'Last')\\\n",
    "    .when(((df_prev_next.time_since_prev < MaxSessionTime) & (df_prev_next.time_till_nxt > MaxSessionTime)), 'Last')\n",
    "    \n",
    "# Apply the above condition on DataFrame and create a column visit_type\n",
    "df_prev_next = df_prev_next.withColumn('visit_type', col_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Sesssion Time\n",
    "\n",
    "##### If visit_type is \"First\", then there must be a visit_type \"Last\" in the next column for a user. In this case Session Time is difference of timestamps of these two columns\n",
    "\n",
    "##### If the visit_type is \"FirstAndLast\", then the SessionTime is 15 mins (with MaxSessionTime assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Window Specification object for IP-wise operations on the data frame\n",
    "window_spec = Window.partitionBy(df_prev_next.client_ip).orderBy(df_prev_next.timestamp)\n",
    "\n",
    "df_prev_next = df_prev_next.withColumn('session_time', df_prev_next.next_timestamp - df_prev_next.timestamp)\n",
    "df_prev_next = df_prev_next.filter(df_prev_next.visit_type != 'Last')\n",
    "\n",
    "df_prev_next = df_prev_next\\\n",
    ".withColumn('session_time_new',func.when(df_prev_next.visit_type != 'FirstAndLast', df_prev_next.session_time).otherwise(MaxSessionTime))\\\n",
    ".drop(df_prev_next.session_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Arrange columns to create sessionization DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sessionized_df = df_prev_next.withColumnRenamed('timestamp','start_time')\\\n",
    "                 .withColumnRenamed('next_timestamp', 'end_time')\\\n",
    "                 .withColumnRenamed('session_time_new', 'session_length')\n",
    "\n",
    "sessionized_df = sessionized_df.select(['client_ip', 'url', 'start_time', 'end_time', 'hour', 'session_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since the log data is not continuous, and the length of maximum continuos data is 7 mins only. The session times, which are 900 secs will give overestimates for average session time as well most engaged users. Also, the expected load in the next minute will be an overestimate. Hence, as a safe assumption, removing all the session times, which are equal to 900 secs. Also removing session times of 0 secs, because it will give an underestimate of average session time and most engaged users. And this time is not usefull for predicting load for next second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionized_df = sessionized_df.filter((sessionized_df.session_length != 900) & (sessionized_df.session_length != 0))\n",
    "sessionized_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1.2 Average Session Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_session_time = sessionized_df.groupBy().avg('session_length').collect()\n",
    "avg_session_time = avg_session_time[0]['avg(session_length)']\n",
    "print('Average Session Time: ' \"{0:.2f}\".format(avg_session_time) + ' sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1.4 Most engaged users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_engaged_users = sessionized_df.groupBy('client_ip').agg(func.sum('session_length').alias('total_session_time'))\n",
    "most_engaged_users = most_engaged_users.orderBy(most_engaged_users.total_session_time.desc())\n",
    "\n",
    "# Print 10 most engaged users in decreasing order of time spent\n",
    "most_engaged_users.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Expected load in next minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_df = log_df.groupBy(['hour', 'minute']).count().alias('total_requests').orderBy('minute').orderBy('hour')\n",
    "load_df = load_df.select(['hour', 'minute', 'count']).orderBy('minute').orderBy('hour')\n",
    "\n",
    "print(\"total_rows: {0}\".format(load_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since the data in load_df doesn't contain large number of rows, transforming this spark DataFrame to pandas DataFrame for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas_df = load_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add column for unique idenfier for hour minute combination\n",
    "pandas_df['hour_minute'] = pandas_df['hour'] * 60 + pandas_df['minute']\n",
    "# Add column for number of request in next minute\n",
    "pandas_df['next_count'] = pandas_df['count'].shift(-1)\n",
    "\n",
    "# Check correlation of load per min with load in next minute and time\n",
    "corr_hour_min_count = pandas_df['count'].corr(pandas_df['hour_minute'])\n",
    "corr_next_count_count = pandas_df['count'].corr(pandas_df['next_count'])\n",
    "print('Correlation with time: ' \"{0:.2f}\".format(corr_hour_min_count))\n",
    "print('Correlation with load in next minute: ' \"{0:.2f}\".format(corr_next_count_count))\n",
    "\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.scatter(pandas_df['hour_minute'], pandas_df['count'])\n",
    "ax1.set_xlabel('hour_minute')\n",
    "ax1.set_ylabel('requests')\n",
    "for k, v in ax1.spines.items():\n",
    "    v.set_visible(False)\n",
    "\n",
    "ax2.scatter(pandas_df['next_count'], pandas_df['count'])\n",
    "ax2.set_xlabel('requests in next minute')\n",
    "ax2.set_ylabel('requests')\n",
    "for k, v in ax2.spines.items():\n",
    "    v.set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### From the plots and correlation values of load with time and load in next minute, we can observe that load in current minute is more correlated with load in next minute. We can use a linear regression model to predict load in next minute based on load in current minute. Load in current minute will act as an independent variable and load in next minute will act as a depedent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove NaN from the DataFrame for regression model\n",
    "pandas_df = pandas_df.loc[pd.notnull(pandas_df['next_count'])]\n",
    "\n",
    "# Create linear regression object\n",
    "regression = linear_model.LinearRegression()\n",
    "\n",
    "X_train = np.transpose(np.array([pandas_df['count']]))\n",
    "Y_train = np.transpose(np.array([pandas_df['next_count']]))\n",
    "\n",
    "# Train the model\n",
    "regression.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "X_test = 10000\n",
    "requests_next_pred = regression.predict(X_test)\n",
    "print(\"Load next minute for current load of {0}: {1:.2f}\".format(X_test, requests_next_pred[0][0]))\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: {0}\".format(regression.coef_))\n",
    "print(\"Intercept: {0}\".format(regression.intercept_))\n",
    "\n",
    "# For plotting linear regression model line\n",
    "x1 = 0\n",
    "x2 = 25000\n",
    "y1 = regression.intercept_[0]\n",
    "y2 = regression.coef_[0][0]*x2 + regression.intercept_[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x1, x2], [y1, y2], color = 'red')\n",
    "ax.scatter(X_train, Y_train)\n",
    "ax.set_xlabel('requests in current minute')\n",
    "ax.set_ylabel('requests in next minute')\n",
    "for k, v in ax.spines.items():\n",
    "    v.set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2.2 Session length for a given IP\n",
    "##### Session length for a given IP is a fixed quantity (I have not understood the predict part here). For a given IP, it can be estimated based on the average of all the session lengths or it can be estimated based on average of session lengths in a particular hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_len_ip_wise = sessionized_df.select(['client_ip', 'hour', 'session_length'])\n",
    "session_len_ip_wise = session_len_ip_wise.groupBy(['client_ip', 'hour']).agg(func.avg('session_length').alias('avg_session_time'))\n",
    "\n",
    "# Printing result for an example IP\n",
    "session_len_ip_wise.filter(session_len_ip_wise.client_ip == '178.255.153.2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Number of unique URL visits by a given IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of unique URLs visited by an IP is also a deterministic value. I may have misunderstood this problem, but computing this based on the data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinct_urls_df = log_df.groupBy(['client_ip']).agg(func.countDistinct('url').alias('distinct_urls'))\n",
    "distinct_urls_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
